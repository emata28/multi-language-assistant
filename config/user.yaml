# User configuration for Draco Assistant

# Speech recognition settings
speech:
  model: "base" # Using base model for good balance of speed and accuracy
  language: null # Set to null to enable auto language detection
  device: null # Audio input device ID (null for default)
  samplerate: 16000
  blocksize: 1024 # Reduced for better real-time handling
  min_speech_duration: 0.3 # Increased minimum duration to avoid false triggers
  silence_duration: 0.5 # Increased silence duration for better phrase detection
  speech_threshold: 0.05 # Increased threshold to avoid background noise
  batch_size: 1 # Process one chunk at a time for faster response
  use_fp16: true # Use half-precision for faster processing
  beam_size: 1 # Reduced beam size for faster processing
  best_of: 1 # Reduced best_of for faster processing
  temperature: 0.0 # Disable sampling for faster processing

# LLM settings
llm:
  provider: "ollama" # LLM provider (ollama)

  # Command classification model (fast and minimal)
  command_model:
    model: "tinyllama" # Fast model for command classification
    temperature: 0.0 # Zero temperature for consistent results
    max_tokens: 1 # Single token for command name
    num_ctx: 4 # Minimal context window for faster processing
    num_thread: 2 # Number of CPU threads to use
    num_gpu: 1 # Number of GPU layers to use (if available)
    top_p: 0.000000001 # Extremely low top_p for most focused outputs
    repeat_penalty: 1.0 # No repetition penalty
    stop: ["Command:", "->", "\n", " "] # Stop at any of these sequences
    num_predict: 1 # Single prediction for command name
    seed: 42 # Fixed seed for consistent outputs

  # Response generation model (more capable)
  response_model:
    model: "llama2" # More capable model for natural responses
    temperature: 0.7 # Higher temperature for more natural responses
    max_tokens: 50 # More tokens for natural responses
    num_ctx: 256 # Larger context window for better understanding
    num_thread: 8 # Number of CPU threads to use
    num_gpu: 3 # Number of GPU layers to use (if available)
    top_p: 0.9 # Higher top_p for more creative responses
    repeat_penalty: 1.1 # Slight repetition penalty
    stop: ["Input:", "Example:", "Command:", "Language:"] # Stop at any of these sequences
    num_predict: 50 # More predictions for natural responses
    seed: 42 # Fixed seed for consistent outputs

# Command settings
commands:
  # System commands (always enabled)
  system:
    enabled: true
    commands:
      - help
      - list
      - status

  # Spotify integration
  spotify:
    enabled: false
    client_id: ""
    client_secret: ""
    redirect_uri: "http://localhost:8888/callback"
    scope: "user-read-playback-state user-modify-playback-state"

  # Smart home integration
  smart_home:
    enabled: false
    host: "http://localhost:8123"
    token: ""
    entities: [] # List of entity IDs to control

# Logging settings
logging:
  level: "INFO" # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
  file: "logs/draco.log" # Log file path
  max_size: 10485760 # Maximum log file size (10MB)
  backup_count: 5 # Number of backup files to keep
